{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180fcdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c991ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoConfig\n",
    "import torch.nn as nn\n",
    "model_name = \"michellejieli/emotion_text_classifier\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = 6\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"michellejieli/emotion_text_classifier\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "37c58274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertForSequenceClassification\n",
    "\n",
    "# # Rebuild model with new head for 6 outputs\n",
    "# from transformers import BertConfig\n",
    "\n",
    "# new_config = BertConfig.from_pretrained(\"michellejieli/emotion_text_classifier\")\n",
    "# new_config.num_labels = 6  # new number of emotion classes\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#     \"michellejieli/emotion_text_classifier\",\n",
    "#     config=new_config,\n",
    "#     ignore_mismatched_sizes=True \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5c96715d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f80a3e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([1, 6])\n",
      "tensor([[-0.4276, -0.2936, -0.5681,  0.1881, -0.3308,  0.2326]])\n"
     ]
    }
   ],
   "source": [
    "#Test how many outputs in the last layer- last layer was modified, changed from 7 to 6 outputs\n",
    "import torch\n",
    "# Test with a dummy input\n",
    "dummy_text = \"This is a test sentence.\"\n",
    "inputs = tokenizer(dummy_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(\"Logits shape:\", outputs.logits.shape)  # Should be (1, 6) for a single input\n",
    "print(outputs.logits)  # Logits for the 6 classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "85c74f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Cleaning text of dataset\n",
    "# # Load dataset\n",
    "# df_combined = pd.read_csv(\n",
    "#     \"final_df_Disgust.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     encoding_errors='ignore',\n",
    "#     low_memory=False,\n",
    "#     dtype={\"label\": int}\n",
    "# )\n",
    "\n",
    "# df = df_combined[['text', 'label']]\n",
    "# df['text'] = df['text'].str.replace(r\"[^/w/s']\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b4e3f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_combined[\"label\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "328c6b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "\n",
    "# #df = pd.read_csv(\"final_df_Disgust.csv\", encoding='latin1')  # Adjust encoding if needed\n",
    "\n",
    "# # Define emotion mapping (ensure labels match your dataset)\n",
    "# emotionMapping = {\n",
    "#     0: 'anger',\n",
    "#     1: 'fear',\n",
    "#     2: 'happy',\n",
    "#     3: 'sad',\n",
    "#     4: 'Disgust',\n",
    "#     5: 'neutral'\n",
    "# }\n",
    "\n",
    "# # Check current class distribution\n",
    "# print(\"Original class distribution:\")\n",
    "# print(df['label'].value_counts().sort_index().rename(emotionMapping))\n",
    "\n",
    "# # --- Step 1: Separate Classes ---\n",
    "# happy = df[df['label'] == 2]  # 'happy'\n",
    "# sad = df[df['label'] == 3]    # 'sad'\n",
    "# anger = df[df['label'] == 0]  # 'anger'\n",
    "# fear = df[df['label'] == 1]   # 'fear'\n",
    "# disgust = df[df['label'] == 4]  # 'Disgust'\n",
    "# neutral = df[df['label'] == 5]  # 'neutral'\n",
    "\n",
    "# # --- Step 2: Hybrid Balancing ---\n",
    "\n",
    "# happy_downsampled = resample(\n",
    "#     happy,\n",
    "#     replace=False,          # No replacement (undersampling)\n",
    "#     n_samples=16971,      # Target size\n",
    "#     random_state=42         # Reproducibility\n",
    "# )\n",
    "\n",
    "# sad_downsampled = resample(\n",
    "#     sad,\n",
    "#     replace=False,\n",
    "#     n_samples=16971,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# disgust_oversampled = resample(\n",
    "#     disgust,\n",
    "#     replace=True,           # Allow replacement (oversampling)\n",
    "#     n_samples=16971,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# neutral_oversampled = resample(\n",
    "#     neutral,\n",
    "#     replace=True,\n",
    "#     n_samples=16971,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # --- Step 3: Combine Balanced Classes --\n",
    "# balanced_df = pd.concat([\n",
    "#     happy_downsampled,\n",
    "#     sad_downsampled,\n",
    "#     anger,\n",
    "#     fear,\n",
    "#     disgust_oversampled,\n",
    "#     neutral_oversampled\n",
    "# ])\n",
    "\n",
    "# # Shuffle the dataset to avoid order bias\n",
    "# balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # --- Step 4: Verify New Distribution ---\n",
    "# print(\"\\nBalanced class distribution:\")\n",
    "# print(balanced_df['label'].value_counts().sort_index().rename(emotionMapping))\n",
    "\n",
    "# # --- Step 5: Save Balanced Dataset (Optional) ---\n",
    "# balanced_df.to_csv(\"balanced_dataset.csv\", index=False)\n",
    "# print(\"\\nBalanced dataset saved to 'balanced_dataset_final_with_Disgust.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedd2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "3    175621\n",
      "5    121187\n",
      "0     57317\n",
      "2     47712\n",
      "4     29359\n",
      "1     28841\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(460037, 2)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"ConcData_Labelled.csv\")\n",
    "df = df[df['label'] != 6]\n",
    "print(df['label'].value_counts())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ce5c586b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "93d13a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3090\n",
      "423734272\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85917449",
   "metadata": {},
   "source": [
    "### Freezing unrequired layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "998bc92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sentiment_analysis_model(df_combined, model_1, tokenizer_1, epochs=10):\n",
    "    df_combined = df_combined.dropna(subset=[\"text\"])\n",
    "    texts = df_combined[\"text\"].astype(str).tolist()\n",
    "    encoded_data = tokenizer_1(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    labels = torch.tensor(df_combined[\"label\"].values, dtype=torch.long)\n",
    "    dataset = TensorDataset(encoded_data[\"input_ids\"], encoded_data[\"attention_mask\"], labels)\n",
    "\n",
    "    train_data, val_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    train_loader = DataLoader(train_data, batch_size=16, shuffle=False)\n",
    "    val_loader = DataLoader(val_data, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = model_1\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0003) #changed from 5e-5 to 1e-4, and from AdamW to Adam\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds, train_labels = [], []\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(outputs.logits.argmax(dim=1).cpu().numpy())  # Predicted labels\n",
    "            train_labels.extend(labels.cpu().numpy())  # Actual labels\n",
    "\n",
    "        train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "        train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(train_labels, train_preds, average='weighted')\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "                val_preds.extend(outputs.logits.argmax(dim=1).cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "        val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(val_labels, val_preds, average='weighted')\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"   Train Loss: {train_loss / len(train_loader):.4f}, Train Acc: {train_accuracy:.4f}, Train F1: {train_f1:.4f}\")\n",
    "        print(f\"   Val Loss: {val_loss / len(val_loader):.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        print(f\"   Train Precision: {train_precision:.4f}, Train Recall: {train_recall:.4f}\")\n",
    "        print(f\"   Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e0194b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12:\n",
      "   Train Loss: 0.2499, Train Acc: 0.9100, Train F1: 0.9099\n",
      "   Val Loss: 0.1984, Val Acc: 0.9263, Val F1: 0.9268\n",
      "   Train Precision: 0.9099, Train Recall: 0.9100\n",
      "   Val Precision: 0.9284, Val Recall: 0.9263\n",
      "------------------------------------------------------------\n",
      "Epoch 2/12:\n",
      "   Train Loss: 0.2420, Train Acc: 0.9123, Train F1: 0.9123\n",
      "   Val Loss: 0.1882, Val Acc: 0.9305, Val F1: 0.9307\n",
      "   Train Precision: 0.9122, Train Recall: 0.9123\n",
      "   Val Precision: 0.9317, Val Recall: 0.9305\n",
      "------------------------------------------------------------\n",
      "Epoch 3/12:\n",
      "   Train Loss: 0.2381, Train Acc: 0.9131, Train F1: 0.9130\n",
      "   Val Loss: 0.1849, Val Acc: 0.9307, Val F1: 0.9311\n",
      "   Train Precision: 0.9129, Train Recall: 0.9131\n",
      "   Val Precision: 0.9322, Val Recall: 0.9307\n",
      "------------------------------------------------------------\n",
      "Epoch 4/12:\n",
      "   Train Loss: 0.2357, Train Acc: 0.9142, Train F1: 0.9141\n",
      "   Val Loss: 0.1796, Val Acc: 0.9323, Val F1: 0.9327\n",
      "   Train Precision: 0.9141, Train Recall: 0.9142\n",
      "   Val Precision: 0.9335, Val Recall: 0.9323\n",
      "------------------------------------------------------------\n",
      "Epoch 5/12:\n",
      "   Train Loss: 0.2345, Train Acc: 0.9148, Train F1: 0.9147\n",
      "   Val Loss: 0.1799, Val Acc: 0.9326, Val F1: 0.9328\n",
      "   Train Precision: 0.9147, Train Recall: 0.9148\n",
      "   Val Precision: 0.9335, Val Recall: 0.9326\n",
      "------------------------------------------------------------\n",
      "Epoch 6/12:\n",
      "   Train Loss: 0.2343, Train Acc: 0.9154, Train F1: 0.9153\n",
      "   Val Loss: 0.1827, Val Acc: 0.9327, Val F1: 0.9329\n",
      "   Train Precision: 0.9153, Train Recall: 0.9154\n",
      "   Val Precision: 0.9337, Val Recall: 0.9327\n",
      "------------------------------------------------------------\n",
      "Epoch 7/12:\n",
      "   Train Loss: 0.2318, Train Acc: 0.9155, Train F1: 0.9154\n",
      "   Val Loss: 0.1767, Val Acc: 0.9339, Val F1: 0.9340\n",
      "   Train Precision: 0.9153, Train Recall: 0.9155\n",
      "   Val Precision: 0.9345, Val Recall: 0.9339\n",
      "------------------------------------------------------------\n",
      "Epoch 8/12:\n",
      "   Train Loss: 0.2324, Train Acc: 0.9154, Train F1: 0.9153\n",
      "   Val Loss: 0.1793, Val Acc: 0.9334, Val F1: 0.9336\n",
      "   Train Precision: 0.9152, Train Recall: 0.9154\n",
      "   Val Precision: 0.9341, Val Recall: 0.9334\n",
      "------------------------------------------------------------\n",
      "Epoch 9/12:\n",
      "   Train Loss: 0.2318, Train Acc: 0.9157, Train F1: 0.9156\n",
      "   Val Loss: 0.1782, Val Acc: 0.9332, Val F1: 0.9336\n",
      "   Train Precision: 0.9156, Train Recall: 0.9157\n",
      "   Val Precision: 0.9343, Val Recall: 0.9332\n",
      "------------------------------------------------------------\n",
      "Epoch 10/12:\n",
      "   Train Loss: 0.2314, Train Acc: 0.9158, Train F1: 0.9157\n",
      "   Val Loss: 0.1779, Val Acc: 0.9337, Val F1: 0.9340\n",
      "   Train Precision: 0.9157, Train Recall: 0.9158\n",
      "   Val Precision: 0.9347, Val Recall: 0.9337\n",
      "------------------------------------------------------------\n",
      "Epoch 11/12:\n",
      "   Train Loss: 0.2297, Train Acc: 0.9165, Train F1: 0.9165\n",
      "   Val Loss: 0.1782, Val Acc: 0.9331, Val F1: 0.9334\n",
      "   Train Precision: 0.9164, Train Recall: 0.9165\n",
      "   Val Precision: 0.9341, Val Recall: 0.9331\n",
      "------------------------------------------------------------\n",
      "Epoch 12/12:\n",
      "   Train Loss: 0.2289, Train Acc: 0.9171, Train F1: 0.9171\n",
      "   Val Loss: 0.1736, Val Acc: 0.9351, Val F1: 0.9354\n",
      "   Train Precision: 0.9170, Train Recall: 0.9171\n",
      "   Val Precision: 0.9360, Val Recall: 0.9351\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train model with improved metrics\n",
    "train_sentiment_analysis_model(df, model, tokenizer, epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e77ebde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newest Model with correct mapping saved to Classifier_withNoSurprise_FineTuned.pth\n"
     ]
    }
   ],
   "source": [
    "# # Save the trained model to a file\n",
    "# model_save_path = \"Classifier_withNoSurprise_FineTuned.pth\"\n",
    "# torch.save(model.state_dict(), model_save_path)\n",
    "# print(f\"Newest Model with correct mapping saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "6f9ecfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at michellejieli/emotion_text_classifier and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([7, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([7]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_15539/2357917484.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"Classifier_withNoSurprise_FineTuned.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To load model\n",
    "model_name=\"michellejieli/emotion_text_classifier\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.num_labels = 6\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.load_state_dict(torch.load(\"Classifier_withNoSurprise_FineTuned.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "f51685cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = {0:\"anger\", 1:\"Disgust\",2:\"fear\",3: \"happy\",4: \"Neutral\", 5: \"sadness\"}\n",
    "\n",
    "def predict_emotion(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Get the corresponding emotion label\n",
    "    # predicted_emotion = emotion_labels[predicted_class]\n",
    "   \n",
    "    return predicted_class, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "eb7973e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"i went to the store and bought some MiLk\"\n",
    "\n",
    "predClass, logits = predict_emotion(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "9f55cf1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.0230407 , -10.251837  ,  -0.86612785,   1.1279086 ,\n",
       "          1.8718865 ,  -0.75101817]], dtype=float32)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaed2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neutral'"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "emotion_labels[predClass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3480527f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
